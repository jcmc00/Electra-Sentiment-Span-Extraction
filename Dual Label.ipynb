{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd()\n",
    "file_dir = Path.cwd()/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(file_dir/'train.csv')\n",
    "train_df.drop(314, inplace = True)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: str(x))\n",
    "train_df['sentiment'] = train_df['sentiment'].apply(lambda x: str(x))\n",
    "train_df['selected_text'] = train_df['selected_text'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(file_dir/'test.csv')\n",
    "test_df['text'] = test_df['text'].apply(lambda x: str(x))\n",
    "test_df['sentiment'] = test_df['sentiment'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['old_sentiment'] = pd.merge(test_df, other_df, left_on='textID', right_on='aux_id', how='left').sentiment_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = Path('/home/jack/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df = pd.read_csv(desktop/'tweet_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['old_sentiment'] = pd.merge(train_df, other_df, left_on='textID', right_on='aux_id', how='left').sentiment_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(other_df[['aux_id','sentiment']], left_on='textID', right_on='aux_id', how='left')\n",
    "train_df.drop('aux_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment_x</th>\n",
       "      <th>sentiment_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27475  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27476  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27477  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27478  ed167662a5                         But it was worth it  ****.   \n",
       "27479  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment_x  \\\n",
       "0                    I`d have responded, if I were going     neutral   \n",
       "1                                               Sooo SAD    negative   \n",
       "2                                            bullying me    negative   \n",
       "3                                         leave me alone    negative   \n",
       "4                                          Sons of ****,    negative   \n",
       "...                                                  ...         ...   \n",
       "27475                                             d lost    negative   \n",
       "27476                                      , don`t force    negative   \n",
       "27477                          Yay good for both of you.    positive   \n",
       "27478                         But it was worth it  ****.    positive   \n",
       "27479  All this flirting going on - The ATG smiles. Y...     neutral   \n",
       "\n",
       "      sentiment_y  \n",
       "0         neutral  \n",
       "1         sadness  \n",
       "2           worry  \n",
       "3           worry  \n",
       "4            hate  \n",
       "...           ...  \n",
       "27475       worry  \n",
       "27476     sadness  \n",
       "27477      relief  \n",
       "27478      relief  \n",
       "27479         fun  \n",
       "\n",
       "[27480 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['new_sentiment'] = train_df.sentiment_x + ' ' + train_df.sentiment_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModel, AutoConfig, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from itertools import compress\n",
    "import utils\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from apex.optimizers.fused_lamb import FusedLAMB as Lamb\n",
    "from ranger import Ranger\n",
    "from fairseq import criterions \n",
    "from functools import partial\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from fastai.core import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_roberta:RobertaTokenizerFast has an issue when working on mask language modeling where it introduces an extra encoded space before the mask token.See https://github.com/huggingface/transformers/pull/2778 for more information.\n"
     ]
    }
   ],
   "source": [
    "max_len = 128\n",
    "bs = 32\n",
    "tokenizer = transformers.RobertaTokenizerFast('input/roberta-base/vocab.json', 'input/roberta-base/merges.txt',\n",
    "                                  lowercase=True,add_prefix_space=True).tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentiment, tweet, selected, tokenizer, max_len):\n",
    "    _input = tokenizer.encode(sentiment, tweet)\n",
    "    _span = tokenizer.encode(selected, add_special_tokens=False)\n",
    "    \n",
    "    len_span = len(_span.ids)\n",
    "    start_idx = None\n",
    "    end_idx = None\n",
    "    \n",
    "    for ind in (i for i, e in enumerate(_input.ids) if e == _span.ids[0]):\n",
    "        if _input.ids[ind: ind + len_span] == _span.ids:\n",
    "            start_idx = ind\n",
    "            end_idx = ind + len_span - 1\n",
    "            break\n",
    "    \n",
    "    # Handles cases where tokenizing input & span separately produces different outputs\n",
    "    if not start_idx:\n",
    "        idx0 = tweet.find(selected)\n",
    "        idx1 = idx0 + len(selected)\n",
    "        \n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1):\n",
    "                char_targets[ct] = 1\n",
    "                \n",
    "        tweet_offsets = _input.offsets[5:-1]\n",
    "        \n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "                \n",
    "        start_idx, end_idx = target_idx[0] + 5 , target_idx[-1] + 5\n",
    "        \n",
    "    _input.start_target = start_idx\n",
    "    _input.end_target = end_idx\n",
    "    _input.tweet = tweet\n",
    "    _input.sentiment = sentiment\n",
    "    _input.selected = selected\n",
    "    \n",
    "    _input.pad(max_len)\n",
    "    \n",
    "    return _input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    \n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return torch.lerp(nll, loss/c, self.ε) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataset, test = None):\n",
    "        self.df = dataset\n",
    "        self.test = test\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.test:\n",
    "            sentiment, tweet, selected = (self.df[col][idx] for col in ['new_sentiment', 'text', 'selected_text'])\n",
    "            _input = preprocess(sentiment, tweet, selected, tokenizer, max_len)\n",
    "            \n",
    "            yb = [torch.tensor(_input.start_target), torch.tensor(_input.end_target)]\n",
    "            \n",
    "        else:\n",
    "            _input = tokenizer.encode(self.df.sentiment[idx], self.df.text[idx])\n",
    "            _input.pad(max_len)\n",
    "            \n",
    "            yb = 0\n",
    "\n",
    "        xb = [torch.LongTensor(_input.ids),\n",
    "              torch.LongTensor(_input.attention_mask),\n",
    "              np.array(_input.offsets)]\n",
    "\n",
    "        return xb, yb     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file input/roberta-base/config.json\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file input/roberta-base/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "roberta_path = 'input/roberta-base'\n",
    "pt_model = AutoModel.from_pretrained(roberta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanModel(nn.Module):\n",
    "    def __init__(self,pt_model):\n",
    "        super().__init__()\n",
    "        self.model = pt_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.qa_outputs1c = torch.nn.Conv1d(768*2, 128, 2)\n",
    "        self.qa_outputs2c = torch.nn.Conv1d(768*2, 128, 2)\n",
    "        self.qa_outputs1 = nn.Linear(128, 1)\n",
    "        self.qa_outputs2 = nn.Linear(128, 1)\n",
    "        \n",
    "#         self.qa_outputs = nn.Linear(768 * 2, 2) # update hidden size\n",
    "\n",
    "    # could pass offsets here and not use - can grab in last_input\n",
    "    def forward(self, input_ids, attention_mask, offsets = None):\n",
    "        \n",
    "        _, _, hidden_states = self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        out = torch.nn.functional.pad(out.transpose(1,2), (1, 0))\n",
    "        \n",
    "        out1 = self.qa_outputs1c(out).transpose(1,2)\n",
    "        out2 = self.qa_outputs2c(out).transpose(1,2)\n",
    "\n",
    "        start_logits = self.qa_outputs1(self.drop_out(out1)).squeeze(-1)\n",
    "        end_logits = self.qa_outputs2(self.drop_out(out2)).squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_between(start_logits, end_logits, device='cpu', max_seq_len=128):\n",
    "    \"\"\"get dist btw. pred & ground_truth\"\"\"\n",
    "\n",
    "    linear_func = torch.tensor(np.linspace(0, 1, max_seq_len, endpoint=False), requires_grad=False)\n",
    "    linear_func = linear_func.to(device)\n",
    "\n",
    "    start_pos = (start_logits*linear_func).sum(axis=1)\n",
    "    end_pos = (end_logits*linear_func).sum(axis=1)\n",
    "\n",
    "    diff = end_pos-start_pos\n",
    "\n",
    "    return diff.sum(axis=0)/diff.size(0)\n",
    "\n",
    "\n",
    "def dist_loss(start_logits, end_logits, start_positions, end_positions, device='cpu', max_seq_len=128, scale=1):\n",
    "    \"\"\"calculate distance loss between prediction's length & GT's length\n",
    "    \n",
    "    Input\n",
    "    - start_logits ; shape (batch, max_seq_len{128})\n",
    "        - logits for start index\n",
    "    - end_logits\n",
    "        - logits for end index\n",
    "    - start_positions ; shape (batch, 1)\n",
    "        - start index for GT\n",
    "    - end_positions\n",
    "        - end index for GT\n",
    "    \"\"\"\n",
    "    start_logits = torch.nn.Softmax(1)(start_logits) # shape ; (batch, max_seq_len)\n",
    "    end_logits = torch.nn.Softmax(1)(end_logits)\n",
    "    \n",
    "    start_one_hot = torch.nn.functional.one_hot(start_positions, num_classes=max_seq_len).to(device)\n",
    "    end_one_hot = torch.nn.functional.one_hot(end_positions, num_classes=max_seq_len).to(device)\n",
    "    \n",
    "    pred_dist = dist_between(start_logits, end_logits, device, max_seq_len)\n",
    "    gt_dist = dist_between(start_one_hot, end_one_hot, device, max_seq_len) # always positive\n",
    "    diff = (gt_dist-pred_dist)\n",
    "\n",
    "    rev_diff_squared = 1-torch.sqrt(diff*diff) # as diff is smaller, make it get closer to the one\n",
    "    loss = -torch.log(rev_diff_squared) # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n",
    "\n",
    "    return loss*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class join_loss(Module):\n",
    "    def __init__(self, ce_loss_fn = nn.CrossEntropyLoss(), device = 'cpu'): \n",
    "        self.loss_fn = ce_loss_fn\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs # assumes tuple input\n",
    "        \n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        \n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        \n",
    "        ce_loss = self.loss_fn(logits, targets)\n",
    "        \n",
    "        distance_loss = dist_loss(start_logits, end_logits, \n",
    "                                  start_targets, end_targets, device=device, max_seq_len =128)\n",
    "        return ce_loss+distance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss(Module):\n",
    "    def __init__(self, loss_fn = nn.CrossEntropyLoss()): \n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs # assumes tuple input\n",
    "        \n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        \n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        \n",
    "        return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_start_end_idxs(start_logits, end_logits):\n",
    "    max_len = len(start_logits)\n",
    "    start_logits, end_logits = start_logits.cpu(),end_logits.cpu()\n",
    "    \n",
    "    a = np.tile(start_logits, (max_len, 1))\n",
    "    b = np.tile(end_logits, (max_len, 1))\n",
    "    c = np.tril(a + b.T, k=0).T\n",
    "    c[c == 0] = -1000\n",
    "    return np.unravel_index(c.argmax(), c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that validation ds is by default not shuffled in fastai - so indexing like this will work for Callback\n",
    "# https://forums.fast.ai/t/how-to-set-shuffle-false-of-train-and-val/33730\n",
    "\n",
    "class JaccardScore(Callback):\n",
    "    \"Stores predictions and targets to perform calculations on epoch end.\"\n",
    "    def __init__(self, valid_ds): \n",
    "        self.valid_ds = valid_ds\n",
    "        self.context_text = valid_ds.df.text\n",
    "        self.answer_text = valid_ds.df.selected_text\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.jaccard_scores = []  \n",
    "        self.valid_ds_idx = 0\n",
    "        \n",
    "        \n",
    "    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "              \n",
    "        input_ids = last_input[0]\n",
    "        offsets = last_input[2]\n",
    "\n",
    "        start_logits, end_logits = last_output\n",
    "        \n",
    "        # for id in batch of ids\n",
    "        for i in range(len(input_ids)):\n",
    "            \n",
    "            _offsets = offsets[i]\n",
    "#             start_idx, end_idx = torch.argmax(start_logits[i]), torch.argmax(end_logits[i])\n",
    "            \n",
    "            start_idx, end_idx = get_best_start_end_idxs(start_logits[i], end_logits[i])\n",
    "            \n",
    "            _answer_text = self.answer_text[self.valid_ds_idx]\n",
    "            original_start, original_end = _offsets[start_idx][0], _offsets[end_idx][1]\n",
    "            pred_span = self.context_text[self.valid_ds_idx][original_start : original_end]\n",
    "                \n",
    "            score = jaccard(pred_span, _answer_text)\n",
    "            self.jaccard_scores.append(score)\n",
    "\n",
    "            self.valid_ds_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):        \n",
    "        res = np.mean(self.jaccard_scores)\n",
    "        return add_metrics(last_metrics, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "\n",
    "def new_on_train_begin(self, **kwargs:Any)->None:\n",
    "    \"Initializes the best value.\"\n",
    "    if not hasattr(self, 'best'):\n",
    "        self.best = float('inf') if self.operator == np.less else -float('inf')\n",
    "\n",
    "SaveModelCallback.on_train_begin = new_on_train_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_indices = list(skf.split(train_df, train_df.new_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "def run_fold(fold):\n",
    "    wd = 0.01 # high value but worked quite well\n",
    "    lr = 1e-4\n",
    "    \n",
    "    tr_df  = train_df.iloc[fold_indices[fold][0]].reset_index(drop=True)\n",
    "    val_df = train_df.iloc[fold_indices[fold][1]].reset_index(drop=True)\n",
    "    \n",
    "    train_ds, valid_ds = [TweetDataset(i) for i in [tr_df,val_df]]\n",
    "    data = DataBunch.create(train_ds, valid_ds,bs = bs)\n",
    "#     loss_fn = partial(CELoss, LabelSmoothingCrossEntropy())\n",
    "    loss_fn = join_loss(LabelSmoothingCrossEntropy(), device) # no need to initialise later - no partial \n",
    "\n",
    "    pt_model = AutoModel.from_pretrained(roberta_path)\n",
    "    model = SpanModel(pt_model)\n",
    "    learner = Learner(data, model, loss_func = loss_fn, opt_func = Ranger, metrics = [JaccardScore(valid_ds)])\n",
    "    \n",
    "    early_stop_cb = EarlyStoppingCallback(learner, monitor='jaccard_score',mode='max',patience=2)\n",
    "    save_model_cb = SaveModelCallback(learner,every='improvement',monitor='jaccard_score',name=f'roberta_conv_fold_{fold}')\n",
    "    csv_logger_cb = CSVLogger(learner, f\"roberta_conv_logs_{fold}\", True)\n",
    "\n",
    "    learner.fit_fc(4, 1e-4,start_pct=0.6, wd=wd, callbacks=[early_stop_cb, save_model_cb, csv_logger_cb])\n",
    "    learner.fit_fc(3, 3e-5,start_pct=0.6, wd=wd, callbacks=[early_stop_cb, save_model_cb, csv_logger_cb])\n",
    "\n",
    "    learner.save(f'roberta_conv_after_epoch_{fold}', with_opt=False)\n",
    "    del learner; del pt_model; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file input/roberta-base/config.json\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file input/roberta-base/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>jaccard_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.605508</td>\n",
       "      <td>1.584496</td>\n",
       "      <td>0.699480</td>\n",
       "      <td>09:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.506651</td>\n",
       "      <td>1.513743</td>\n",
       "      <td>0.704634</td>\n",
       "      <td>10:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.436278</td>\n",
       "      <td>1.543316</td>\n",
       "      <td>0.695403</td>\n",
       "      <td>09:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.318321</td>\n",
       "      <td>1.512942</td>\n",
       "      <td>0.708374</td>\n",
       "      <td>10:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with jaccard_score value: 0.6994798050652977.\n",
      "Better model found at epoch 1 with jaccard_score value: 0.7046335294632677.\n",
      "Better model found at epoch 3 with jaccard_score value: 0.7083736517595187.\n",
      "set state called\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [2/3 19:37<09:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>jaccard_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.310541</td>\n",
       "      <td>1.533289</td>\n",
       "      <td>0.701220</td>\n",
       "      <td>09:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.276020</td>\n",
       "      <td>1.573133</td>\n",
       "      <td>0.692638</td>\n",
       "      <td>09:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.23% [9/172 00:03<01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ranger FC RUN\n",
    "for fold in range(1,5):run_fold(fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
